Tyler Amos : Tamos5
Justin Canedy : Jcanedy1

/* Experiments */
We decided to use the gcc.trace file for our experiments and keep that constant so that the cache configuration
was the only independent variable in our experiements. We also kept the size of the data constant at 16 bytes

**NOTE: The eviction policy chosen makes a non-significant change in miss/hit/cycle counts in comparison to the other configurations, so we will primarily
consider LRU to compare the other configurations.

// write-allocate write-back lru

**general**
1st we found that larger caches have significantly less misses and total cycles. However, it comes at the cost of time. 
Since we do a linear scan of the sets to find if there are valid blocks left, when the cache becomes full, we scan through the whole set.
Then we scan through the whole set again to update all of the orders. For every single querey, this causes the time to increase towards
the end of the program. 

1 256 16
Then we tried a fully associative cache, which ran incredibly slow and cost a significant amount of total cycles (10802483). This is because we had to access the cpu
very often because after the cache was full, we had to write to the cpu every time a query was made because a block was definitely going to be evicted.
If the cache was huge, it would take less time and would cost less cycles, but that is impractical. 

256 1 16
Then we tried a direct-mapped cache and it ran extremely fast, but it cost way more cycles (20312483) than the associative cache becuase we had to access the cpu 
on eviction more often. Since all of these configurations are write-allocate, we have to access the CPU on every store miss, which is expensive.

256 4 16
A 4-way set associative cache is our base case for what we presumed would be optimal with this configuration and all configurations (obviously).
This cost the least total cycles (9344483) because when the data is split into multiple sets, with multiple available blocks in every set, there are less evictions
which means that the cpu is not accessed as often. The store/load hit/miss count is all the same, but the cycles are what changes due to cpu accesses occuring less
frequent. 

** From this point on we will only consider a 4-way associative cache (256 4 16) because we know that it is more optimal ** 
// write-allocate write-through lru
write-through was way more expensive in total cycles than write-back (25318283 vs 9344483). This is because every single store accesses the cpu. 
Files with a lot of loads may benefit more from this configuration, but trace files with a mix of both can become expensive. 
The count on the load/store hits/misses was the same as in write-back, suggesting that simply the cost of accessing the cpu on every store
is more expensive and less efficient than write-back.


// no-write-allocate write-though lru
This configuration is relatively close to write-allocate write-through in terms of total cycles (22865216), outperforming it slightly.
However, there are significantly more store misses (32667 vs 9236) and load misses (6584 vs 3399). This makes sense since the no-write-allocate configuration bypasses writing to the cache and writes sraight to memory. This means that only loads misses cause memory to be stored in the cache. This is supported by the fact that no-write-allocate had more load misses than write-allocate.
It appears that given a mixture of loads and stores both configurations perform similarly in terms of cycles.

// write-allocate write-back lru
This configuration had the best overall performance with cycles being 9344483. It had lower store/load misses than no-write-allocate write-through and equal store/load misses to write-allocate write-through. This suggests that the drastic difference in cycles is due to the write-back configuration and how store hits are processed. Unlike write-through which writes to cache and main mem every time, write-back only writes to the cache and only to main mem if the block has been editted. This can cause better performance for multiple reasons. First, if the block brought in undergoes many writes, then write-allocate must write to main mem on every write, whereas write-back may only write to main mem once if that block is evicted. Secondly, write-allocate and write-back incur the same cost if the block brought in to cache only undergoes 1 write since both will write to main mem once. So in the worst case, write-back is equivalent to write-through. As for the write-allocate configuration, we saw above that it has little difference in performance when compared to no-write-allocate given a mixture of stores and loads. Thus write-allocate write-back is the best performing configuration.


/* Contributions */
Tyler:
In general for this project, I set up a lot of the general framework, then Justin would fix all of the bugs and perfect the way the the program ran. 
Justin and I frequently communicated about what stage our program was at, and suggested to each other possible parts of the code to check to hone in on
more specific bugs. Justin coded an amazing output mechanism that displayed our whole cache, what was being queried, and the update after it was attempted
to be added to the cache. I used that to set up the first stages of the eviction policies, which Justin later debugged and perfected. Essentially we took turns
working on the project, and over the course of 1.5 weeks constructed the whole program in tandem. Justin would implement something new and I would debug it, 
then I would implement something new and Justin would debug it. We didn't necessarily assign specific aspects of the project to each other, but were fluid
in adding to the program. S/O Justin for finding most of our major bugs and fixing them! I also handled all of the parameter checking. 







Justin:
Indeed Tyler did a majority of the cache/set/block framework whereas I created a majority of the function definitions(The actual code in the functions was split between us).
Tyler created the bulk of csim and its error handeling/retrieval of input and I wrote the code to parse each address into its tag/index bits. The homework as a whole was a major
team effort as Tyler mentioned. It was a constant back and forth of one of us not knowing how to do something at the time and the other debugging it. A large part of it came from implementing
the different configurations and evictions when one of us would write new code, get stuck, and then the other would fix it. 






